{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing (updating) the following libraries for your Sagemaker\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ..  # installing d2l\n",
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "![输入表示](../img/bert_inputs.svg)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class BERTEncoder(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, pw_num_hiddens, num_heads,\n",
    "                 num_layers, dropout, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.segment_embedding = nn.Embedding(2, embed_size)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(embed_size, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for _ in range(num_layers):\n",
    "            self.blks.add(d2l.EncoderBlock(\n",
    "                embed_size, pw_num_hiddens, num_heads, dropout))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_length):\n",
    "        # Shape of X remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, embed_size)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = self.pos_encoding(X)\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_length)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, embed_size, pw_num_hiddens = 10000, 768, 1024\n",
    "num_heads, num_layers, dropout = 4, 2, 0.1\n",
    "encoder = BERTEncoder(vocab_size, embed_size, pw_num_hiddens, num_heads,\n",
    "                      num_layers, dropout)\n",
    "encoder.initialize()\n",
    "tokens = np.random.randint(0, 10000, (2, 8))\n",
    "segments = np.array([[0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                     [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encodings = encoder(tokens, segments, None)\n",
    "encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "![双向语言模型](../img/biLM_Leakage.svg)\n",
    "\n",
    "...\n",
    "![遮蔽语言模型](../img/bert_mlm.svg)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class MaskLMDecoder(nn.Block):\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLMDecoder, self).__init__(**kwargs)\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add(\n",
    "            nn.Dense(num_hiddens, flatten=False, activation='relu'))\n",
    "        self.decoder.add(nn.LayerNorm())\n",
    "        self.decoder.add(nn.Dense(vocab_size, flatten=False))\n",
    "\n",
    "    def forward(self, X, masked_positions):\n",
    "        num_masked_positions = masked_positions.shape[1]\n",
    "        masked_positions = masked_positions.reshape((1, -1))\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = np.arange(0, batch_size)   \n",
    "        batch_idx = np.repeat(batch_idx, num_masked_positions)\n",
    "        batch_idx = np.expand_dims(batch_idx, axis=0)\n",
    "        encoded = X[batch_idx, masked_positions]\n",
    "        encoded = encoded.reshape((batch_size, num_masked_positions, X.shape[-1]))\n",
    "        pred = self.decoder(encoded)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.1954829   1.291365    0.1002758  ...  0.1350491   0.75628126\n",
      "   -0.04147708]\n",
      "  [ 1.3979957   1.0108925   0.25892815 ... -0.0502919   0.8459401\n",
      "   -0.1025135 ]\n",
      "  [ 1.457378    0.5411348   0.27132604 ... -0.18223824  0.94110763\n",
      "   -0.13618413]\n",
      "  ...\n",
      "  [ 0.43992355  0.39078015  0.05209163 ... -0.29429975  0.9758662\n",
      "   -0.36763987]\n",
      "  [ 0.6068441   0.5813843   0.3420515  ... -0.2207006   0.97231364\n",
      "   -0.50001675]\n",
      "  [ 0.8938728   0.5913229   0.573172   ... -0.00974531  1.0499427\n",
      "   -0.4945277 ]]\n",
      "\n",
      " [[ 1.227987    1.3020387   0.11307816 ...  0.15336904  0.73382765\n",
      "   -0.08130519]\n",
      "  [ 1.4514396   0.96184206  0.26130652 ...  0.04136756  0.86831087\n",
      "   -0.14407723]\n",
      "  [ 1.3674635   0.5616111   0.2968637  ... -0.20094985  0.9411952\n",
      "   -0.27811664]\n",
      "  ...\n",
      "  [ 0.5336172   0.3034801   0.08665738 ... -0.28749323  0.96964234\n",
      "   -0.364873  ]\n",
      "  [ 0.6454773   0.51735216  0.37236986 ... -0.15298626  1.0318402\n",
      "   -0.5390184 ]\n",
      "  [ 0.9145006   0.52909255  0.6179646  ...  0.01441541  0.9860765\n",
      "   -0.5185786 ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encodings)\n",
    "\n",
    "mlm_decoder = MaskLMDecoder(vocab_size, embed_size)\n",
    "mlm_decoder.initialize()\n",
    "\n",
    "mlm_positions = np.array([[0, 1], [4, 8]])\n",
    "mlm_label = np.array([[100, 200], [100, 200]])\n",
    "mlm_pred = mlm_decoder(encodings, mlm_positions)\n",
    "mlm_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "mlm_loss = mlm_loss_fn(mlm_pred, mlm_label)\n",
    "mlm_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "![下一句预测](../img/bert_nsp.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class NextSentenceClassifier(nn.Block):\n",
    "    def __init__(self, num_hiddens, **kwargs):\n",
    "        super(NextSentenceClassifier, self).__init__(**kwargs)\n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add(nn.Dense(num_hiddens, flatten=False,\n",
    "                                     activation='tanh'))\n",
    "        self.classifier.add(nn.Dense(2, flatten=False))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[:, 0, :]\n",
    "        return self.classifier(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) (2,)\n"
     ]
    }
   ],
   "source": [
    "ns_classifier = NextSentenceClassifier(embed_size)\n",
    "ns_classifier.initialize()\n",
    "\n",
    "ns_pred = ns_classifier(encodings)\n",
    "ns_label = np.array([0, 1])\n",
    "ns_loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "ns_loss = ns_loss_fn(ns_pred, ns_label)\n",
    "print(ns_pred.shape, ns_loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class BERTModel(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, pw_num_hiddens, num_heads,\n",
    "                 num_layers, dropout):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, embed_size, pw_num_hiddens,\n",
    "                                   num_heads, num_layers, dropout)\n",
    "        self.ns_classifier = NextSentenceClassifier(embed_size)\n",
    "        self.mlm_decoder = MaskLMDecoder(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None, masked_positions=None):\n",
    "        seq_out = self.encoder(inputs, token_types, valid_length)\n",
    "        next_sentence_classifier_out = self.ns_classifier(seq_out)\n",
    "        if not masked_positions is None:\n",
    "            mlm_decoder_out = self.mlm_decoder(seq_out, masked_positions)\n",
    "        else:\n",
    "            mlm_decoder_out = None\n",
    "        return seq_out, next_sentence_classifier_out, mlm_decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}