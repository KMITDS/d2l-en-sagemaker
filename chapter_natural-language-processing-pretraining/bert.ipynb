{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing (updating) the following libraries for your Sagemaker\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ..  # installing d2l\n",
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "![输入表示](../img/bert_inputs.svg)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class BERTEncoder(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, pw_num_hiddens, num_heads,\n",
    "                 num_layers, dropout, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = gluon.nn.Embedding(vocab_size, embed_size)\n",
    "        self.segment_embedding = gluon.nn.Embedding(2, embed_size)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(embed_size, dropout)\n",
    "        self.blks = gluon.nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(d2l.EncoderBlock(\n",
    "                embed_size, pw_num_hiddens, num_heads, dropout))\n",
    "\n",
    "    def forward(self, tokens, segments, mask):\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = self.pos_encoding(X)\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8, 768)\n"
     ]
    }
   ],
   "source": [
    "encoder = BERTEncoder(vocab_size=10000, embed_size=768, pw_num_hiddens=1024,\n",
    "                      num_heads=4, num_layers=2, dropout=0.1)\n",
    "encoder.initialize()\n",
    "num_samples, num_tokens = 2, 8\n",
    "tokens = np.random.randint(0, 10000, (2, 8))\n",
    "segments = np.array([[0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                     [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encodings = encoder(tokens, segments, None)\n",
    "print(encodings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "![双向语言模型](../img/biLM_Leakage.svg)\n",
    "\n",
    "...\n",
    "![遮蔽语言模型](../img/bert_mlm.svg)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class MaskLMDecoder(nn.Block):\n",
    "    def __init__(self, vocab_size, units, **kwargs):\n",
    "        super(MaskLMDecoder, self).__init__(**kwargs)\n",
    "        self.decoder = gluon.nn.Sequential()\n",
    "        self.decoder.add(gluon.nn.Dense(units, flatten=False, activation='relu'))\n",
    "        self.decoder.add(gluon.nn.LayerNorm())\n",
    "        self.decoder.add(gluon.nn.Dense(vocab_size, flatten=False))\n",
    "\n",
    "    def forward(self, X, masked_positions):\n",
    "        ctx = masked_positions.context\n",
    "        dtype = masked_positions.dtype\n",
    "        num_masked_positions = masked_positions.shape[1]\n",
    "        masked_positions = masked_positions.reshape((1, -1))\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = np.arange(0, batch_size)\n",
    "        batch_idx = np.repeat(batch_idx, num_masked_positions)\n",
    "        batch_idx = batch_idx.reshape((1, -1))\n",
    "        encoded = X[batch_idx, masked_positions]\n",
    "        encoded = encoded.reshape((batch_size, num_masked_positions, X.shape[-1]))\n",
    "        pred = self.decoder(encoded)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 10000) (2,)\n"
     ]
    }
   ],
   "source": [
    "mlm_decoder = MaskLMDecoder(vocab_size=10000, units=768)\n",
    "mlm_decoder.initialize()\n",
    "\n",
    "mlm_positions = np.array([[0, 1], [4, 8]])\n",
    "mlm_label = np.array([[100, 200], [100, 200]])\n",
    "mlm_pred = mlm_decoder(encodings, mlm_positions)\n",
    "mlm_loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "mlm_loss = mlm_loss_fn(mlm_pred, mlm_label)\n",
    "print(mlm_pred.shape, mlm_loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "![下一句预测](../img/bert_nsp.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class NextSentenceClassifier(nn.Block):\n",
    "    def __init__(self, units=768, **kwargs):\n",
    "        super(NextSentenceClassifier, self).__init__(**kwargs)\n",
    "        self.classifier = gluon.nn.Sequential()\n",
    "        self.classifier.add(gluon.nn.Dense(units=units, flatten=False,\n",
    "                                           activation='tanh'))\n",
    "        self.classifier.add(gluon.nn.Dense(units=2, flatten=False))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[:, 0, :]\n",
    "        return self.classifier(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) (2,)\n"
     ]
    }
   ],
   "source": [
    "ns_classifier = NextSentenceClassifier()\n",
    "ns_classifier.initialize()\n",
    "\n",
    "ns_pred = ns_classifier(encodings)\n",
    "ns_label = np.array([0, 1])\n",
    "ns_loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "ns_loss = ns_loss_fn(ns_pred, ns_label)\n",
    "print(ns_pred.shape, ns_loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class BERTModel(nn.Block):\n",
    "    def __init__(self, vocab_size=None, embed_size=128, pw_num_hiddens=512,\n",
    "                 num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size=vocab_size, embed_size=embed_size, pw_num_hiddens=pw_num_hiddens,\n",
    "                                   num_heads=num_heads, num_layers=num_layers, dropout=dropout)\n",
    "        self.ns_classifier = NextSentenceClassifier()\n",
    "        self.mlm_decoder = MaskLMDecoder(vocab_size=vocab_size, units=embed_size)\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None, masked_positions=None):\n",
    "        seq_out = self.encoder(inputs, token_types, valid_length)\n",
    "        next_sentence_classifier_out = self.ns_classifier(seq_out)\n",
    "        if not masked_positions is None:\n",
    "            mlm_decoder_out = self.mlm_decoder(seq_out, masked_positions)\n",
    "        else:\n",
    "            mlm_decoder_out = None\n",
    "        return seq_out, next_sentence_classifier_out, mlm_decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}