{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing (updating) the following libraries for your Sagemaker\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ..  # installing d2l\n",
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "We have introduced several word embedding models for natural language understanding.\n",
    "After pretraining, the output can be thought of as a matrix\n",
    "where each row is a vector that represents a word of a predefined vocabulary.\n",
    "In fact, these word embedding models are all *context-independent*.\n",
    "Let's begin by illustrating this property.\n",
    "\n",
    "\n",
    "## From Context-Independent to Context-Sensitive\n",
    "\n",
    "Recall the experiments in :numref:`sec_word2vec_gluon` and :numref:`sec_synonyms`.\n",
    "For instance, word2vec and GloVe both assign the same pretrained vector to the same word regardless of the context of the word (if any).\n",
    "Given the abundance of polysemy and complex semantics in natural languages,\n",
    "context-independent representations have obvious limitations.\n",
    "For instance, the word \"crane\" in contexts\n",
    "\"a crane is flying\" and \"a crane driver came\" has completely different meanings;\n",
    "thus, the same word may be assigned different representations depending on contexts.\n",
    "\n",
    "This motivates the development of *context-sensitive* word representations,\n",
    "where the representation of words depend on their contexts.\n",
    "For example, by taking the entire sequence as the input,\n",
    "ELMo (Embeddings from Language Models) is a function that assigns a representation to each word from the input sequence :cite:`Peters.Neumann.Iyyer.ea.2018`.\n",
    "\n",
    "\n",
    "Specifically, ELMo combines all the intermediate layer representations from pretrained bidirectional LSTM as the output representation.\n",
    "Then the ELMo representation will be added to a downstream task's existing supervised model\n",
    "as additional features, such as by concatenating ELMo representation and the original representation (e.g., GloVe) of tokens in the existing model.\n",
    "On one hand,\n",
    "all the weights in the pretrained bidirectional LSTM model are frozen after ELMo representations are added.\n",
    "On the other hand,\n",
    "the existing supervised model is task specific.\n",
    "Leveraging different best models for different tasks at that time,\n",
    "adding ELMo improved the state of the art across $6$ NLP tasks.\n",
    "\n",
    "\n",
    "## From Task-Specific to Task-Agnostic\n",
    "\n",
    "Although ELMo has significantly improved solutions to diverse NLP tasks,\n",
    "each solution is still customized to each task.\n",
    "However, it is practically non-trivial to craft a specific architecture for every NLP task.\n",
    "The GPT (Generative Pre-Training) model represents an effort in designing\n",
    "a general task-agnostic model for context-sensitive representations :cite:`Radford.Narasimhan.Salimans.ea.2018`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class BERTEncoder(nn.Block):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_layers, dropout, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for _ in range(num_layers):\n",
    "            self.blks.add(d2l.EncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of X remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, num_hiddens)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = self.pos_encoding(X)\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens = 10000, 768, 1024\n",
    "num_heads, num_layers, dropout = 4, 2, 0.1\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                      num_layers, dropout)\n",
    "encoder.initialize()\n",
    "tokens = np.random.randint(0, 10000, (2, 8))\n",
    "segments = np.array([[0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                     [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class MaskLM(nn.Block):\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential()\n",
    "        self.mlp.add(\n",
    "            nn.Dense(num_hiddens, flatten=False, activation='relu'))\n",
    "        self.mlp.add(nn.LayerNorm())\n",
    "        self.mlp.add(nn.Dense(vocab_size, flatten=False))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = np.arange(0, batch_size)\n",
    "        # Suppose that batch_size = 2, num_pred_positions = 3,\n",
    "        # batch_idx = np.array([0, 0, 0, 1, 1, 1])\n",
    "        batch_idx = np.repeat(batch_idx, num_pred_positions)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm.initialize()\n",
    "mlm_positions = np.array([[0, 2, 1], [6, 5, 7]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y = np.array([[1, 3, 5], [10, 20, 30]])\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class NextSentencePred(nn.Block):\n",
    "    def __init__(self, num_hiddens, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential()\n",
    "        self.mlp.add(nn.Dense(num_hiddens, activation='tanh'))\n",
    "        self.mlp.add(nn.Dense(2))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 0 is the index of the CLS token\n",
    "        X = X[:, 0, :]\n",
    "        # X shape: (batch size, num_hiddens)\n",
    "        return self.mlp(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp = NextSentencePred(num_hiddens)\n",
    "nsp.initialize()\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_y = np.array([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class BERTModel(nn.Block):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_layers, dropout):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                                   num_heads, num_layers, dropout)\n",
    "        self.nsp = NextSentencePred(num_hiddens)\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None,\n",
    "                pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        nsp_Y_hat = self.nsp(encoded_X)\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}