{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Pair Embedding (WordPiece)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "vocabs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "          'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "original_words = {'low_' : 5, 'lower_' : 2, 'newest_' : 6, 'widest_' : 3}\n",
    "words = {}\n",
    "for word, freq in original_words.items():\n",
    "    new_word = ' '.join(list(word))\n",
    "    words[new_word] = original_words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_freq_pair(words):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in words.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Occurrence of each adjacent unit\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    max_freq_pair = max(pairs, key = pairs.get)\n",
    "    return max_freq_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def merge_vocab(max_freq_pair, words, vocabs):\n",
    "    bigram = ' '.join(max_freq_pair)\n",
    "    vocabs.append(''.join(max_freq_pair))\n",
    "    words_out = {}\n",
    "    for word, freq in words.items():\n",
    "        new_word = word.replace(bigram, ''.join(max_freq_pair))\n",
    "        words_out[new_word] = words[word]\n",
    "    return words_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge #1: ('e', 's')\n",
      "Merge #2: ('es', 't')\n",
      "Merge #3: ('est', '_')\n",
      "Merge #4: ('l', 'o')\n",
      "Merge #5: ('lo', 'w')\n",
      "Merge #6: ('n', 'e')\n",
      "Merge #7: ('ne', 'w')\n",
      "Merge #8: ('new', 'est_')\n",
      "Merge #9: ('low', '_')\n",
      "Merge #10: ('w', 'i')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(words)\n",
    "    words = merge_vocab(max_freq_pair, words, vocabs)\n",
    "    print(\"Merge #%d:\" % (i + 1), max_freq_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['low_', 'lower_', 'newest_', 'widest_']\n",
      "Wordpieces: ['low_', 'low e r _', 'newest_', 'wi d est_']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words:\", list(original_words.keys()))\n",
    "print(\"Wordpieces:\", list(words.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabs: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'es', 'est', 'est_', 'lo', 'low', 'ne', 'new', 'newest_', 'low_', 'wi']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabs:\", vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['slow_', 'slowest_']\n",
      "Wordpieces: ['s low_', 's low est_']\n"
     ]
    }
   ],
   "source": [
    "inputs = ['slow_', 'slowest_']\n",
    "outputs = []\n",
    "for word in inputs:\n",
    "    start, end = 0, len(word)\n",
    "    cur_output = []\n",
    "    while start < len(word) and start < end:\n",
    "        if word[start : end] in vocabs:\n",
    "            cur_output.append(word[start : end])\n",
    "            start = end\n",
    "            end = len(word)\n",
    "        else:\n",
    "            end -= 1\n",
    "    if start < len(word):\n",
    "        cur_output.append('[UNK]')\n",
    "    outputs.append(' '.join(cur_output))\n",
    "print('Words:', inputs)\n",
    "print('Wordpieces:', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "...\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... & Klingner, J. (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.\n",
    "\n",
    "[2] Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}