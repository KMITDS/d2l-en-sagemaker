{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook on Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install d2l\n",
    "!pip install -U\n",
    "!pip install --pre\n",
    "!pip install mxnet-cu101mkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Sentiment Classification: Using Recurrent Neural Networks\n",
    "\n",
    ":label:`sec_sentiment_rnn`\n",
    "\n",
    "\n",
    "\n",
    "Similar to search synonyms and analogies, text classification is also a\n",
    "downstream application of word embedding. In this section, we will apply\n",
    "pre-trained word vectors and bidirectional recurrent neural networks with\n",
    "multiple hidden layers :cite:`Maas.Daly.Pham.ea.2011`. We will use them to\n",
    "determine whether a text sequence of indefinite length contains positive or\n",
    "negative emotion. Import the required package or module before starting the\n",
    "experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, init, np, npx\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.contrib import text\n",
    "npx.set_np()\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Recurrent Neural Network Model\n",
    "\n",
    "In this model, each word first obtains a feature vector from the embedding\n",
    "layer. Then, we further encode the feature sequence using a bidirectional\n",
    "recurrent neural network to obtain sequence information. Finally, we transform\n",
    "the encoded sequence information to output through the fully connected\n",
    "layer. Specifically, we can concatenate hidden states of bidirectional\n",
    "long-short term memory in the initial timestep and final timestep and pass it\n",
    "to the output layer classification as encoded feature sequence information. In\n",
    "the `BiRNN` class implemented below, the `Embedding` instance is the embedding\n",
    "layer, the `LSTM` instance is the hidden layer for sequence encoding, and the\n",
    "`Dense` instance is the output layer for generated classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "46"
    }
   },
   "outputs": [],
   "source": [
    "class BiRNN(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Set Bidirectional to True to get a bidirectional recurrent neural\n",
    "        # network\n",
    "        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True, input_size=embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The shape of inputs is (batch size, number of words). Because LSTM\n",
    "        # needs to use sequence as the first dimension, the input is\n",
    "        # transformed and the word feature is then extracted. The output shape\n",
    "        # is (number of words, batch size, word vector dimension).\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        # Since the input (embeddings) is the only argument passed into\n",
    "        # rnn.LSTM, it only returns the hidden states of the last hidden layer\n",
    "        # at different timestep (outputs). The shape of outputs is\n",
    "        # (number of words, batch size, 2 * number of hidden units).\n",
    "        outputs = self.encoder(embeddings)\n",
    "        # Concatenate the hidden states of the initial timestep and final\n",
    "        # timestep to use as the input of the fully connected layer. Its\n",
    "        # shape is (batch size, 4 * number of hidden units)\n",
    "        encoding = np.concatenate((outputs[0], outputs[-1]), axis=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bidirectional recurrent neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, ctx = 100, 100, 2, d2l.try_all_gpus()\n",
    "net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)\n",
    "net.initialize(init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Word Vectors\n",
    "\n",
    "Because the training dataset for sentiment classification is not very large, in order to deal with overfitting, we will directly use word vectors pre-trained on a larger corpus as the feature vectors of all words. Here, we load a 100-dimensional GloVe word vector for each word in the dictionary `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = text.embedding.create(\n",
    "    'glove', pretrained_file_name='glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the word vectors that in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = glove_embedding.get_vecs_by_tokens(vocab.idx_to_token)\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use these word vectors as feature vectors for each word in the reviews. Note that the dimensions of the pre-trained word vectors need to be consistent with the embedding layer output size `embed_size` in the created model. In addition, we no longer update these word vectors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "47"
    }
   },
   "outputs": [],
   "source": [
    "net.embedding.weight.set_data(embeds)\n",
    "net.embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating the Model\n",
    "\n",
    "Now, we can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "48"
    }
   },
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "49"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def predict_sentiment(net, vocab, sentence):\n",
    "    sentence = np.array(vocab[sentence.split()], ctx=d2l.try_gpu())\n",
    "    label = np.argmax(net(sentence.reshape(1, -1)), axis=1)\n",
    "    return 'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the trained model to classify the sentiments of two simple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "50"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Text classification transforms a sequence of text of indefinite length into a category of text. This is a downstream application of word embedding.\n",
    "* We can apply pre-trained word vectors and recurrent neural networks to classify the emotions in a text.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Increase the number of epochs. What accuracy rate can you achieve on the training and testing datasets? What about trying to re-tune other hyper-parameters?\n",
    "1. Will using larger pre-trained word vectors, such as 300-dimensional GloVe word vectors, improve classification accuracy?\n",
    "1. Can we improve the classification accuracy by using the spaCy word tokenization tool? You need to install spaCy: `pip install spacy` and install the English package: `python -m spacy download en`. In the code, first import spacy: `import spacy`. Then, load the spacy English package: `spacy_en = spacy.load('en')`. Finally, define the function `def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]` and replace the original `tokenizer` function. It should be noted that GloVe's word vector uses \"-\" to connect each word when storing noun phrases. For example, the phrase \"new york\" is represented as \"new-york\" in GloVe. After using spaCy tokenization, \"new york\" may be stored as \"new york\".\n",
    "\n",
    "\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/2391)\n",
    "\n",
    "![](../img/qr_sentiment-analysis-rnn.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}