{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing (updating) the following libraries for your Sagemaker\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n",
    "!pip install ..  # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting House Prices on Kaggle\n",
    "\n",
    ":label:`sec_kaggle_house`\n",
    "\n",
    "\n",
    "Now that we have introduced some basic tools\n",
    "for building and training deep networks\n",
    "and regularizing them with techniques including\n",
    "dimensionality reduction, weight decay, and dropout,\n",
    "we are ready to put all this knowledge into practice\n",
    "by participating in a Kaggle competition.\n",
    "[Predicting house prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) is a great place to start.\n",
    "The data is fairly generic and does not exhibit exotic structure\n",
    "that might require specialized models (as audio or video might).\n",
    "This dataset, collected by Bart de Cock in 2011 :cite:`De-Cock.2011`,\n",
    "covers house prices in Ames, IA from the period of 2006-2010.\n",
    "It is considerably larger than the famous [Boston housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names) of Harrison and Rubinfeld (1978),\n",
    "boasting both more examples and more features.\n",
    "\n",
    "\n",
    "In this section, we will walk you through details of \n",
    "data preprocessing, model design, and hyperparameter selection.\n",
    "We hope that through a hands-on approach,\n",
    "you will gain some intuitions that will guide you\n",
    "in your career as a data scientist.\n",
    "\n",
    "\n",
    "## Downloading and Caching Datasets\n",
    "\n",
    "Throughout the book, we will train and test models \n",
    "on various downloaded datasets. \n",
    "Here, we implement several utility functions \n",
    "to facilitate data downloading. \n",
    "First, we maintain a dictionary `DATA_HUB` \n",
    "that maps a string (the *name* of the dataset)\n",
    "to a tuple containing both a URL to locate the dataset\n",
    "and a SHA-1 key which we will use \n",
    "to verify the integrity of the file. \n",
    "All of our datasets are hosted on site\n",
    "whose address is assigned to `DATA_URL` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mxnet import gluon\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "# Saved in the d2l package for later use\n",
    "DATA_HUB = dict()\n",
    "\n",
    "# Saved in the d2l package for later use\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `download` function downloads the dataset,\n",
    "caching it in a local directory (in `../data` by default)\n",
    "and returns the name of the downloaded file.\n",
    "If a file corrsponding to this dataset \n",
    "already exists in the cache directory \n",
    "and its SHA-1 matches the one stored in `DATA_HUB`, \n",
    "our code will use the cached file to avoid \n",
    "clogging up your internet with redundant downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def download(name, cache_dir=os.path.join('..', 'data')):\n",
    "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
    "    assert name in DATA_HUB, \"%s doesn't exist\" % name\n",
    "    url, sha1 = DATA_HUB[name]\n",
    "    d2l.mkdir_if_not_exist(cache_dir)\n",
    "    return gluon.utils.download(url, cache_dir, sha1_hash=sha1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement two additional functions: \n",
    "one is to download and extract a zip/tar file\n",
    "and the other to download all the files from `DATA_HUB`\n",
    "(most of the datasets used in this book) into the cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def download_extract(name, folder=None):\n",
    "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname) \n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == '.zip':\n",
    "        fp = zipfile.ZipFile(fname, 'r')\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        fp = tarfile.open(fname, 'r')\n",
    "    else:\n",
    "        assert False, 'Only zip/tar files can be extracted'\n",
    "    fp.extractall(base_dir)\n",
    "    if folder:\n",
    "        return os.path.join(base_dir, folder)\n",
    "    else:\n",
    "        return data_dir\n",
    "\n",
    "# Saved in the d2l package for later use\n",
    "def download_all():\n",
    "    \"\"\"Download all files in the DATA_HUB\"\"\"\n",
    "    for name in DATA_HUB:\n",
    "        download(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle\n",
    "\n",
    "[Kaggle](https://www.kaggle.com) is a popular platform\n",
    "that hosts machine learning competitions.\n",
    "Each competition centers on a dataset and many\n",
    "are sponsored by stakeholders who offer prizes\n",
    "to the winning solutions.\n",
    "The platform helps users to share interact \n",
    "via forums and shared code, \n",
    "fostering both collaboration and competition.\n",
    "While leaderboard chasing often spirals out of control,\n",
    "with researchers focusing myopically on pre-processing steps\n",
    "rather than asking fundamental questions,\n",
    "there's also tremendous value in the objectivity of a platform\n",
    "that facillitates direct quantitative comparisons\n",
    "between competing approaches as well as code sharing\n",
    "so that everyone can learn what did and did not work.\n",
    "If you want to participate in a Kaggle competitions,\n",
    "you will first need to register for an account\n",
    "(see :numref:`fig_kaggle`).\n",
    "\n",
    "![Kaggle website](../img/kaggle.png)\n",
    "\n",
    ":width:`400px`\n",
    "\n",
    "\n",
    ":label:`fig_kaggle`\n",
    "\n",
    "\n",
    "On the House Prices Prediction page, as illustrated \n",
    "in :numref:`fig_house_pricing`,\n",
    "you can find the dataset (under the \"Data\" tab),\n",
    "submit predictions, see your ranking, etc.,\n",
    "The URL is right here:\n",
    "\n",
    "> https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "![House Price Prediction](../img/house_pricing.png)\n",
    "\n",
    ":width:`400px`\n",
    "\n",
    "\n",
    ":label:`fig_house_pricing`\n",
    "\n",
    "\n",
    "## Accessing and Reading the Dataset\n",
    "\n",
    "Note that the competition data is separated \n",
    "into training and test sets.\n",
    "Each record includes the property value of the house\n",
    "and attributes such as street type, year of construction,\n",
    "roof type, basement condition, etc.\n",
    "The features consist of various data types.\n",
    "For example, the year of construction\n",
    "is represented by an integer,\n",
    "the roof type by discrete categorical assignments,\n",
    "and other features by floating point numbers.\n",
    "And here is where reality complicates things:\n",
    "for some examples, some data is altogether missing\n",
    "with the missing value marked simply as *na*.\n",
    "The price of each house is included \n",
    "for the training set only\n",
    "(it is a competition after all).\n",
    "We will want to partition the training set\n",
    "to create a validation set,\n",
    "but we only get to evaluate our models on the official test set\n",
    "after uploading predictions to Kaggle.\n",
    "The \"Data\" tab on the competition tab \n",
    "has links to download the data.\n",
    "\n",
    "\n",
    "To get started, we will read in and process the data\n",
    "using `pandas`, an [efficient data analysis toolkit](http://pandas.pydata.org/pandas-docs/stable/),\n",
    "so you will want to make sure that you have `pandas` installed\n",
    "before proceeding further.\n",
    "Fortunately, if you are reading in Jupyter,\n",
    "we can install pandas without even leaving the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "# If pandas is not installed, please uncomment the following line:\n",
    "# !pip install pandas\n",
    "\n",
    "%matplotlib inline\n",
    "import d2l\n",
    "from mxnet import autograd, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "import pandas as pd\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can download and cache \n",
    "the Kaggle housing dataset \n",
    "using the script we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use        \n",
    "DATA_HUB['kaggle_house_train'] = (\n",
    "    DATA_URL + 'kaggle_house_pred_train.csv',\n",
    "    '585e9cc93e70b39160e7921475f9bcd7d31219ce')\n",
    "\n",
    "# Saved in the d2l package for later use  \n",
    "DATA_HUB['kaggle_house_test'] = (\n",
    "    DATA_URL + 'kaggle_house_pred_test.csv',\n",
    "    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the two csv files containing training \n",
    "and test data respectively we use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(download('kaggle_house_train'))\n",
    "test_data = pd.read_csv(download('kaggle_house_test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset includes $1460$ examples, \n",
    "$80$ features, and $1$ label, while the test data \n",
    "contains $1459$ examples and $80$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the first $4$ and last $2$ features\n",
    "as well as the label (SalePrice) from the first $4$ examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in each example, the first feature is the ID.\n",
    "This helps the model identify each training example.\n",
    "While this is convenient, it does not carry\n",
    "any information for prediction purposes.\n",
    "Hence, we remove it from the dataset \n",
    "before feeding the data into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As stated above, we have a wide variety of data types.\n",
    "We'll need to process the data before we can start modeling.\n",
    "Let's start with the numerical features.\n",
    "First, we apply a heuristic, \n",
    "replacing all missing values \n",
    "by the corresponding variable's mean.\n",
    "Then, to put all variables on a common scale,\n",
    "we rescale them to zero mean and unit variance:\n",
    "\n",
    "$$x \\leftarrow \\frac{x - \\mu}{\\sigma}.$$\n",
    "\n",
    "To verify that this indeed transforms\n",
    "our variable such that it has zero mean and unit variance,\n",
    "note that $E[(x-\\mu)/\\sigma] = (\\mu - \\mu)/\\sigma = 0$\n",
    "and that $E[(x-\\mu)^2] = \\sigma^2$.\n",
    "Intuitively, we *normalize* the data\n",
    "for two reasons. \n",
    "First, it proves convenient for optimization.\n",
    "Second, because we do not know *a priori*\n",
    "which features will be relevant,\n",
    "we do not want to penalize coefficients\n",
    "assigned to one variable more than on any other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# After standardizing the data all means vanish, hence we can set missing\n",
    "# values to 0\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we deal with discrete values.\n",
    "This includes variables such as 'MSZoning'.\n",
    "We replace them by a one-hot encoding\n",
    "in the same way that we previously transformed\n",
    "multiclass labels into vectors.\n",
    "For instance, 'MSZoning' assumes the values 'RL' and 'RM'.\n",
    "These map onto vectors $(1, 0)$ and $(0, 1)$ respectively.\n",
    "Pandas does this automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# Dummy_na=True refers to a missing value being a legal eigenvalue, and\n",
    "# creates an indicative feature for it\n",
    "all_features = pd.get_dummies(all_features, dummy_na=True)\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this conversion increases \n",
    "the number of features from 79 to 331.\n",
    "Finally, via the `values` attribute,\n",
    "we can extract the NumPy format from the Pandas dataframe\n",
    "and convert it into MXNet's native `ndarray` \n",
    "representation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "n_train = train_data.shape[0]\n",
    "train_features = np.array(all_features[:n_train].values, dtype=np.float32)\n",
    "test_features = np.array(all_features[n_train:].values, dtype=np.float32)\n",
    "train_labels = np.array(train_data.SalePrice.values,\n",
    "                        dtype=np.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To get started we train a linear model with squared loss.\n",
    "Not surprisingly, our linear model will not lead\n",
    "to a competition-winning submission\n",
    "but it provides a sanity check to see whether\n",
    "there is meaningful information in the data.\n",
    "If we cannot do better than random guessing here,\n",
    "then there might be a good chance\n",
    "that we have a data processing bug.\n",
    "And if things work, the linear model will serve as a baseline\n",
    "giving us some intuition about how close the simple model\n",
    "gets to the best reported models, giving us a sense\n",
    "of how much gain we should expect from fancier models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.L2Loss()\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With house prices, as with stock prices,\n",
    "we care about relative quantities \n",
    "more than absolute quantities.\n",
    "Thus we tend to care more about \n",
    "the relative error $\\frac{y - \\hat{y}}{y}$\n",
    "than about the absolute error $y - \\hat{y}$.\n",
    "For instance, if our prediction is off by USD 100,000\n",
    "when estimating the price of a house in Rural Ohio,\n",
    "where the value of a typical house is 125,000 USD,\n",
    "then we are probably doing a horrible job.\n",
    "On the other hand, if we err by this amount \n",
    "in Los Altos Hills, California,\n",
    "this might represent a stunningly accurate prediction\n",
    "(there, the median house price exceeds 4 million USD).\n",
    "\n",
    "One way to address this problem is to\n",
    "measure the discrepancy in the logarithm of the price estimates.\n",
    "In fact, this is also the official error metric\n",
    "used by the competition to measure the quality of submissions.\n",
    "After all, a small value $\\delta$ of $\\log y - \\log \\hat{y}$\n",
    "translates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$.\n",
    "This leads to the following loss function:\n",
    "\n",
    "$$L = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "def log_rmse(net, features, labels):\n",
    "    # To further stabilize the value when the logarithm is taken, set the\n",
    "    # value less than 1 as 1\n",
    "    clipped_preds = np.clip(net(features), 1, float('inf'))\n",
    "    return np.sqrt(2 * loss(np.log(clipped_preds), np.log(labels)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in previous sections, our training functions \n",
    "will rely on the Adam optimizer\n",
    "(a slight variant on SGD that we will describe \n",
    "in greater detail later).\n",
    "The main appeal of Adam vs vanilla SGD \n",
    "is that the Adam optimizer, \n",
    "despite doing no better (and sometimes worse)\n",
    "given unlimited resources for hyperparameter optimization,\n",
    "people tend to find that it is significantly less sensitive\n",
    "to the initial learning rate.\n",
    "This will be covered in further detail later on\n",
    "when we discuss the details in :numref:`chap_optimization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_features, train_labels, test_features, test_labels,\n",
    "          num_epochs, learning_rate, weight_decay, batch_size):\n",
    "    train_ls, test_ls = [], []\n",
    "    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n",
    "    # The Adam optimization algorithm is used here\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {\n",
    "        'learning_rate': learning_rate, 'wd': weight_decay})\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
    "    return train_ls, test_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Fold Cross-Validation\n",
    "\n",
    "If you are reading in a linear fashion,\n",
    "you might recall that we introduced k-fold cross-validation\n",
    "in the section where we discussed how to deal\n",
    "with model section (:numref:`sec_model_selection`).\n",
    "We will put this to good use to select the model design\n",
    "and to adjust the hyperparameters.\n",
    "We first need a function that returns\n",
    "the $i^\\mathrm{th}$ fold of the data\n",
    "in a k-fold cross-validation procedure.\n",
    "It proceeds by slicing out the $i^\\mathrm{th}$ segment\n",
    "as validation data and returning the rest as training data.\n",
    "Note that this is not the most efficient way of handling data\n",
    "and we would definitely do something much smarter\n",
    "if our dataset was considerably larger.\n",
    "But this added complexity might obfuscate our code unnecessarily\n",
    "so we can safely omit here owing to the simplicity of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X_part), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_part), axis=0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and verification error averages are returned\n",
    "when we train $k$ times in the k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs,\n",
    "           learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        if i == 0:\n",
    "            d2l.plot(list(range(1, num_epochs+1)), [train_ls, valid_ls],\n",
    "                     xlabel='epoch', ylabel='rmse',\n",
    "                     legend=['train', 'valid'], yscale='log')\n",
    "        print('fold %d, train rmse: %f, valid rmse: %f' % (\n",
    "            i, train_ls[-1], valid_ls[-1]))\n",
    "    return train_l_sum / k, valid_l_sum / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "In this example, we pick an untuned set of hyperparameters\n",
    "and leave it up to the reader to improve the model.\n",
    "Finding a good choice can take time,\n",
    "depending on how many variables one optimizes over.\n",
    "With a large enough dataset, \n",
    "and the normal sorts of hyperparameters reason,\n",
    "k-fold cross-validation tends to be \n",
    "reasonably resilient against multiple testing.\n",
    "However, if we try an unreasonably large number of options\n",
    "we might just get lucky and find that our validation\n",
    "performance is no longer representative of the true error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n",
    "train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\n",
    "                          weight_decay, batch_size)\n",
    "print('%d-fold validation: avg train rmse: %f, avg valid rmse: %f'\n",
    "      % (k, train_l, valid_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that someimes the number of training errors\n",
    "for a set of hyperparameters can be very low,\n",
    "even as the number of errors on $k$-fold cross-validation \n",
    "is considerably higher. \n",
    "This indicates that we are overfitting.\n",
    "Throughout training you will want to monitor both numbers.\n",
    "No overfitting might indicate that our data can support a more powerful model. \n",
    "Massive overfitting might suggest that we can gain \n",
    "by incorporating regularization techniques.\n",
    "\n",
    "##  Predict and Submit\n",
    "\n",
    "Now that we know what a good choice of hyperparameters should be,\n",
    "we might as well use all the data to train on it\n",
    "(rather than just $1-1/k$ of the data\n",
    "that is used in the cross-validation slices).\n",
    "The model that we obtain in this way\n",
    "can then be applied to the test set.\n",
    "Saving the estimates in a CSV file\n",
    "will simplify uploading the results to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_pred(train_features, test_feature, train_labels, test_data,\n",
    "                   num_epochs, lr, weight_decay, batch_size):\n",
    "    net = get_net()\n",
    "    train_ls, _ = train(net, train_features, train_labels, None, None,\n",
    "                        num_epochs, lr, weight_decay, batch_size)\n",
    "    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',\n",
    "             ylabel='rmse', yscale='log')\n",
    "    print('train rmse %f' % train_ls[-1])\n",
    "    # Apply the network to the test set\n",
    "    preds = net(test_features).asnumpy()\n",
    "    # Reformat it for export to Kaggle\n",
    "    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice sanity check is to see\n",
    "whether the predictions on the test set\n",
    "resemble those of the k-fold cross-validation process.\n",
    "If they do, it is time to upload them to Kaggle.\n",
    "The following code will generate a file called `submission.csv`\n",
    "(CSV is one of the file formats accepted by Kaggle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "train_and_pred(train_features, test_features, train_labels, test_data,\n",
    "               num_epochs, lr, weight_decay, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as demonstrated in :numref:`fig_kaggle_submit2`,\n",
    "we can submit our predictions on Kaggle\n",
    "and see how they compare to the actual house prices (labels) \n",
    "on the test set.\n",
    "The steps are quite simple:\n",
    "\n",
    "* Log in to the Kaggle website and visit the House Price Prediction Competition page.\n",
    "* Click the “Submit Predictions” or “Late Submission” button (as of this writing, the button is located on the right).\n",
    "* Click the “Upload Submission File” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n",
    "* Click the “Make Submission” button at the bottom of the page to view your results.\n",
    "\n",
    "![Submitting data to Kaggle](../img/kaggle_submit2.png)\n",
    "\n",
    ":width:`400px`\n",
    "\n",
    "\n",
    ":label:`fig_kaggle_submit2`\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Real data often contains a mix of different data types and needs to be preprocessed.\n",
    "* Rescaling real-valued data to zero mean and unit variance is a good default. So is replacing missing values with their mean.\n",
    "* Transforming categorical variables into indicator variables allows us to treat them like vectors.\n",
    "* We can use k-fold cross validation to select the model and adjust the hyper-parameters.\n",
    "* Logarithms are useful for relative loss.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Submit your predictions for this tutorial to Kaggle. How good are your predictions?\n",
    "1. Can you improve your model by minimizing the log-price directly? What happens if you try to predict the log price rather than the price?\n",
    "1. Is it always a good idea to replace missing values by their mean? Hint: can you construct a situation where the values are not missing at random?\n",
    "1. Find a better representation to deal with missing values. Hint: what happens if you add an indicator variable?\n",
    "1. Improve the score on Kaggle by tuning the hyperparameters through k-fold cross-validation.\n",
    "1. Improve the score by improving the model (layers, regularization, dropout).\n",
    "1. What happens if we do not standardize the continuous numerical features like we have done in this section?\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/2346)\n",
    "\n",
    "![](../img/qr_kaggle-house-price.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}